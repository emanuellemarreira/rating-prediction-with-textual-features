{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EGRJuZzvuq29",
        "outputId": "e721087f-c74c-4527-e4b2-45b0ada2ed4f"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install numpy\n",
        "!pip install senticnet\n",
        "!apt-get update\n",
        "!apt-get install -y hunspell libhunspell-dev\n",
        "!pip install hunspell==0.5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEaNEYaBK5Gr",
        "outputId": "0d46384f-6987-4218-818c-174590bc4af3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy.cli.download(\"pt_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDv1TLGnRcoo"
      },
      "outputs": [],
      "source": [
        "import csv, re, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import pt_core_news_sm\n",
        "import csv\n",
        "from senticnet.babelsenticnet import BabelSenticNet as SenticNet\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import cross_validate\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import hunspell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),'..'))\n",
        "words_dir = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','palavras.txt')\n",
        "liwc_dir = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','LIWC')\n",
        "lexico_path = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','SentiProdBR')\n",
        "emoticon_dir = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','emoticon_sentiment_lexicon.tsv')\n",
        "emoji_dir = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','emoji_ranking.csv')\n",
        "girias_dir = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','girias.txt')\n",
        "dic_path = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','Dic. Hunspell', 'pt_BR.dic.txt')\n",
        "aff_path = os.path.join(PROJECT_ROOT, 'feature_extraction', 'res','Dic. Hunspell', 'pt_BR.aff.txt')\n",
        "extract_path = os.path.join(PROJECT_ROOT, 'dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eRArD3cpHgN"
      },
      "outputs": [],
      "source": [
        "feature_names = [\n",
        "'qtCharacters',\n",
        "'qtSentences',\n",
        "'qtWords',\n",
        "'qtCapitalizedWords',\n",
        "'qtCapitalizedChars',\n",
        "'propCapitalizedWords',\n",
        "'propCapitalizedChars',\n",
        "'POS_adjectives',\n",
        "'POS_adp',\n",
        "'POS_adv',\n",
        "'POS_aux',\n",
        "'POS_cconj',\n",
        "'POS_det',\n",
        "'POS_intj',\n",
        "'POS_noun',\n",
        "'POS_num',\n",
        "'POS_part',\n",
        "'POS_pron',\n",
        "'POS_propn',\n",
        "'POS_punct',\n",
        "'POS_sconj',\n",
        "'POS_sym',\n",
        "'POS_verb',\n",
        "'POS_comp',\n",
        "'POS_sup',\n",
        "'POS_x',\n",
        "'SYNT_1',\n",
        "'SYNT_2',\n",
        "'SYNT_3',\n",
        "'SYNT_4',\n",
        "'SYNT_5',\n",
        "'LEX_Subjective',\n",
        "'LEX_PropSubjective',\n",
        "'LEX_Positive',\n",
        "'LEX_PropPositive',\n",
        "'LEX_Negative',\n",
        "'LEX_PropNegative',\n",
        "'LEX_CountNegativeSentences',\n",
        "'LEX_CountPositiveSentences',\n",
        "'LEX_QuotationExclamation',\n",
        "'LEX_PropQuotationExclamation',\n",
        "'CONC_Pleasantness',\n",
        "'CONC_AvgPleasantness',\n",
        "'CONC_Attention',\n",
        "'CONC_AvgAttention',\n",
        "'CONC_Sensitivity',\n",
        "'CONC_AvgSensitivity',\n",
        "'CONC_Aptitude',\n",
        "'CONC_AvgAptitude',\n",
        "'CONC_Polarity',\n",
        "'CONC_AvgPolarity',\n",
        "'TWT_CountElongated',\n",
        "'TWT_CountExpressions',\n",
        "'TWT_Negation',\n",
        "'TWT_EmojiPolarityScore',\n",
        "'TWT_EmoticonPolarityScore',\n",
        "'SBJ_CountNE',\n",
        "#'SBJ_FutureTense', feature retirada\n",
        "'SBJ_CountCorrectWords'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZVxnk50yryf"
      },
      "outputs": [],
      "source": [
        "class SpacyPreprocessor(BaseEstimator):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        return list(self.nlp.pipe(sentences, n_process = 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYtu9iBHC-sL"
      },
      "source": [
        "# Structural Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--YTWMTDZLqn"
      },
      "outputs": [],
      "source": [
        "class CountCharacters(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"CHARACTERS\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for sentence in sentences:\n",
        "          list_count.append(len(sentence))\n",
        "        return np.array(list_count).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su_K-Y8Ml6wG"
      },
      "outputs": [],
      "source": [
        "class CountSentences(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"SENTENCES\"\n",
        "        self.nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        list_count = []\n",
        "        for document in documents:\n",
        "            sentences = list(self.nlp(document).sents)\n",
        "            list_count.append(len(sentences))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUvomSFEgygw"
      },
      "outputs": [],
      "source": [
        "class CountWords(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"WORDS\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        tokenized_sentences = []\n",
        "        for sentence in sentences:\n",
        "            words = len(re.findall(r'[^\\s!\\?,\\(\\)\\.]+', sentence))\n",
        "            tokenized_sentences.append(words)\n",
        "        return np.array(tokenized_sentences).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXSS7SyOGsUW"
      },
      "outputs": [],
      "source": [
        "class CountWordsWithUpperLetter(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"COUNT OF CAPITALIZED WORDS\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len(re.findall(r'[A-Z]\\w*', doc)))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wu_4DygDCD8"
      },
      "outputs": [],
      "source": [
        "class CountUpperLetters(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"COUNT OF CAPITALIZED CHARACTERS\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            words = re.findall(r'([A-Z])', doc)\n",
        "            list_count.append(len(words))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgQmK96UI10c"
      },
      "outputs": [],
      "source": [
        "class ProportionCapitalizedWords(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"Ratio of words with upper letters to lower letters\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            word_count = len(re.findall(r'[^\\s!\\?,\\(\\)\\.]+', doc))\n",
        "            capitalized_count = len(re.findall(r'[A-Z]\\w*', doc))\n",
        "            list_count.append(0 if capitalized_count == 0 else capitalized_count/word_count)\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7G2BWtrLKAG"
      },
      "outputs": [],
      "source": [
        "class ProportionCapitalizedChars(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"Ratio of the uppercase characters to the sentence length\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            words = re.findall(r'([A-Z])', doc)\n",
        "            list_count.append(0 if len(words) == 0 else len(words) / len(doc))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Xe5dM339G-"
      },
      "source": [
        "# POS Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiB2HRZQfELA"
      },
      "outputs": [],
      "source": [
        "class POS_CountAdj(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"ADJ\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'ADJ']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRrwdjPEsSkk"
      },
      "outputs": [],
      "source": [
        "class POS_CountAdp(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"ADP\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'ADP']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COjGgBCI_Vmm"
      },
      "outputs": [],
      "source": [
        "class POS_CountAdv(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"ADV\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'ADV']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-3tW2OUsoHa"
      },
      "outputs": [],
      "source": [
        "class POS_CountAux(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"AUX\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'AUX']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCNqIpvOsu4y"
      },
      "outputs": [],
      "source": [
        "class POS_CountCconj(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"CCONJ\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'CCONJ']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwyJiRdYtGMX"
      },
      "outputs": [],
      "source": [
        "class POS_CountDet(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"DET\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'DET']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwHyTopQtLuS"
      },
      "outputs": [],
      "source": [
        "class POS_CountIntj(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"INTJ\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'INTJ']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUkfENOjAhYS"
      },
      "outputs": [],
      "source": [
        "class POS_CountNoun(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"NOUN\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'NOUN']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cg3Tqn6rxl2"
      },
      "outputs": [],
      "source": [
        "class POS_CountNum(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"NUM\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'NUM']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnO2tcDVuOc7"
      },
      "outputs": [],
      "source": [
        "class POS_CountPart(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"PART\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'PART']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDToo4VxugYM"
      },
      "outputs": [],
      "source": [
        "class POS_CountPron(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"PRON\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'PRON']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGIxxPbEujxN"
      },
      "outputs": [],
      "source": [
        "class POS_CountPropn(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"PROPN\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'PROPN']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbIzCiggBDcw"
      },
      "outputs": [],
      "source": [
        "class POS_CountPunct(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"ADP\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'PUNCT']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeYE37JsunKR"
      },
      "outputs": [],
      "source": [
        "class POS_CountSconj(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"SCONJ\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'SCONJ']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJM_Jm0MvcPb"
      },
      "outputs": [],
      "source": [
        "class POS_CountSym(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"SYM\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'SYM']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3fBlEwlvg7s"
      },
      "outputs": [],
      "source": [
        "class POS_CountVerb(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"VERB\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'VERB']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3AD8X_BehvbN"
      },
      "outputs": [],
      "source": [
        "class POS_CountComparatives(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"Contar comparativos\"\n",
        "        self.sinteticos = ['melhor','pior','maior','menor']\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def comparativo_igualdade(self, sentence):\n",
        "      count_comparatives = 0\n",
        "      for i in range(len(sentence)):\n",
        "        if i + 2 < len(sentence):\n",
        "          if sentence[i].text.lower() == \"tão\":\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              if sentence[i+2].text.lower() == \"quanto\" or sentence[i+2].text.lower() == \"como\" or sentence[i+2].text.lower() == \"quão\" :\n",
        "                  count_comparatives += 1\n",
        "      return count_comparatives\n",
        "\n",
        "    def comparativo_superioridade(self, sentence):\n",
        "      count_comparatives = 0\n",
        "      for i in range(len(sentence)):\n",
        "        if i + 3 < len(sentence):\n",
        "          if sentence[i].text.lower() == \"mais\":\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              if sentence[i+2].text.lower() == \"que\" or (sentence[i+2].text.lower() == \"do\" and sentence[i+3].text.lower() == \"que\") :\n",
        "                  count_comparatives += 1\n",
        "      return count_comparatives\n",
        "\n",
        "    def comparativo_inferioridade(self, sentence):\n",
        "      count_comparatives = 0\n",
        "      for i in range(len(sentence)):\n",
        "        if i + 3 < len(sentence):\n",
        "          if sentence[i].text.lower() == \"menos\":\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              if sentence[i+2].text.lower() == \"que\" or (sentence[i+2].text.lower() == \"do\" and sentence[i+3].text.lower() == \"que\") :\n",
        "                  count_comparatives += 1\n",
        "      return count_comparatives\n",
        "\n",
        "    def comparativo_sintetico(self, sentence):\n",
        "      count_comparatives = 0\n",
        "      for i in range(len(sentence)):\n",
        "        if i + 2 < len(sentence):\n",
        "          if sentence[i].text.lower() in self.sinteticos:\n",
        "            if sentence[i+1].text.lower() == \"que\" or (sentence[i+1].text.lower() == \"do\" and sentence[i+2].text.lower() == \"que\") :\n",
        "                count_comparatives += 1\n",
        "      return count_comparatives\n",
        "\n",
        "\n",
        "    def transform(self, sentences):\n",
        "         list_count = []\n",
        "         for doc in sentences:\n",
        "             list_count.append(self.comparativo_igualdade(doc)\n",
        "                              + self.comparativo_superioridade(doc)\n",
        "                              + self.comparativo_inferioridade(doc)\n",
        "                              + self.comparativo_sintetico(doc))\n",
        "         return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGxQYjyHgXje"
      },
      "outputs": [],
      "source": [
        "class POS_CountSuperlatives(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"Contar superlativos\"\n",
        "        self.nlp = SpacyPreprocessor().nlp\n",
        "        self.artigos = ['o','a','os','as']\n",
        "        self.intensificadores = ['muito', 'extremamente', 'completamente', 'totalmente', 'bastante', 'incrivelmente', 'super', 'mega', 'hiper']\n",
        "        self.sufixos = ['íssim', 'im', 'ílim', 'érrim']\n",
        "\n",
        "    def superlativo_relativo_inferioridade(self, sentence):\n",
        "      count_superlatives, i = 0,0\n",
        "      while i <= len(sentence):\n",
        "        if i+3 < len(sentence):\n",
        "          if sentence[i].text.lower() in self.artigos:\n",
        "            if sentence[i+1].pos_ == \"NOUN\" and sentence[i+2].text.lower() == \"menos\" and sentence[i+3].pos_ == \"ADJ\":\n",
        "              count_superlatives += 1\n",
        "              i += 4\n",
        "        if i+2 < len(sentence):\n",
        "          if sentence[i].text.lower() in self.artigos:\n",
        "            if sentence[i+1].text.lower() == \"menos\" and sentence[i+2].pos_ == \"ADJ\":\n",
        "              count_superlatives += 1\n",
        "              i += 3\n",
        "        i += 1\n",
        "      return count_superlatives\n",
        "\n",
        "    def superlativo_relativo_superioridade(self, sentence):\n",
        "      count_superlatives, i = 0,0\n",
        "      while i <= len(sentence):\n",
        "        if i+3 < len(sentence):\n",
        "          if sentence[i].text.lower() in self.artigos:\n",
        "            if sentence[i+1].pos_ == \"NOUN\" and sentence[i+2].text.lower() == \"mais\" and sentence[i+3].pos_ == \"ADJ\":\n",
        "              count_superlatives += 1\n",
        "              i += 4\n",
        "        if i+2 < len(sentence):\n",
        "          if sentence[i].text.lower() in self.artigos:\n",
        "            if sentence[i+1].text.lower() == \"mais\" and sentence[i+2].pos_ == \"ADJ\":\n",
        "              count_superlatives += 1\n",
        "              i += 3\n",
        "        i += 1\n",
        "      return count_superlatives\n",
        "\n",
        "    def superlativo_absoluto_analitico(self, sentence):\n",
        "      count_superlatives = 0\n",
        "      for i in range(len(sentence)):\n",
        "        if i+1 < len(sentence):\n",
        "          if sentence[i].text.lower() in self.intensificadores:\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              count_superlatives += 1\n",
        "      return count_superlatives\n",
        "\n",
        "    def superlativo_absoluto_sintetico(self, sentence):\n",
        "      count_superlatives = 0\n",
        "      for token in sentence:\n",
        "        base = token.lemma_.lower() if token.lemma_ else token.text.lower()\n",
        "        token_base = next(self.nlp.pipe([base]))[0]\n",
        "        if token_base.pos_ == \"ADJ\":\n",
        "           for sufixo in self.sufixos:\n",
        "              if token.text.lower().endswith(sufixo + 'o') or token.text.lower().endswith(sufixo + 'a') or token.text.lower().endswith(sufixo + 'os') or token.text.lower().endswith(sufixo + 'as'):\n",
        "                count_superlatives += 1\n",
        "                break\n",
        "      return count_superlatives\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "         list_count = []\n",
        "         for doc in sentences:\n",
        "             list_count.append(self.superlativo_relativo_inferioridade(doc)\n",
        "                              + self.superlativo_relativo_superioridade(doc)\n",
        "                              + self.superlativo_absoluto_analitico(doc)\n",
        "                              + self.superlativo_absoluto_sintetico(doc))\n",
        "         return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gThF-RhvkC7"
      },
      "outputs": [],
      "source": [
        "class POS_CountX(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"X\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len([token for token in doc if token.pos_ == 'X']))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLMXUC_V4Jvx"
      },
      "source": [
        "# Syntactic Rules Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Ib5ilh5hfR"
      },
      "outputs": [],
      "source": [
        "# ADJ -> NOUN -> *\n",
        "class SYNT_Rule1(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"RULE 1\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def __pattern__(self, sentence):\n",
        "      for i in range(len(sentence)):\n",
        "        if i+1 < len(sentence):\n",
        "          if sentence[i].pos_ == \"ADJ\":\n",
        "            if sentence[i+1].pos_ == \"NOUN\":\n",
        "              return 1\n",
        "      return 0\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__pattern__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04vipPgFSoGI"
      },
      "outputs": [],
      "source": [
        "# ADV + [<SUP> | <COMP>] -> ADJ -> !NOUN\n",
        "class SYNT_Rule2(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"RULE 2\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def __pattern__(self, sentence):\n",
        "      for i in range(len(sentence)):\n",
        "        if i+2 < len(sentence):\n",
        "          if sentence[i].pos_ == \"ADV\" or (sentence[i].pos_ == \"ADV\" and \"<SUP>\" in sentence[i].tag_) or (sentence[i].pos_ == \"ADV\" and \"<COMP>\" in sentence[i].tag_):\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              if sentence[i+2].pos_ != \"NOUN\":\n",
        "                return 1\n",
        "      return 0\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__pattern__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m0sA0icTePx"
      },
      "outputs": [],
      "source": [
        "# ADJ -> ADJ -> !NOUN\n",
        "class SYNT_Rule3(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"RULE 3\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def __pattern__(self, sentence):\n",
        "      for i in range(len(sentence)):\n",
        "        if i+2 < len(sentence):\n",
        "          if sentence[i].pos_ == \"ADJ\":\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              if sentence[i+2].pos_ != \"NOUN\":\n",
        "                return 1\n",
        "      return 0\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__pattern__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KAZ6LWKTryx"
      },
      "outputs": [],
      "source": [
        "# NOUN -> ADJ -> !NOUN\n",
        "class SYNT_Rule4(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"RULE 4\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def __pattern__(self, sentence):\n",
        "      for i in range(len(sentence)):\n",
        "        if i+2 < len(sentence):\n",
        "          if sentence[i].pos_ == \"NOUN\":\n",
        "            if sentence[i+1].pos_ == \"ADJ\":\n",
        "              if sentence[i+2].pos_ != \"NOUN\":\n",
        "                return 1\n",
        "      return 0\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__pattern__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDBQ2N7ZUMYs"
      },
      "outputs": [],
      "source": [
        "# ADV + [<SUP> | <COMP>] -> VERB + [ PCP | GER | PS | IMPF]  -> *\n",
        "class SYNT_Rule5(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"RULE 5\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def __pattern__(self, sentence):\n",
        "      for i in range(len(sentence)):\n",
        "        if i+1 < len(sentence):\n",
        "          if sentence[i].pos_ == \"ADV\" or (sentence[i].pos_ == \"ADV\" and \"<SUP>\" in sentence[i].tag_) or (sentence[i].pos_ == \"ADV\" and \"<COMP>\" in sentence[i].tag_):\n",
        "            if sentence[i+1].pos_ == \"VERB\" or (sentence[i].pos_ == \"VERB\" and \"<PCP>\" in sentence[i].tag_) or (sentence[i].pos_ == \"VERB\" and \"<GER>\" in sentence[i].tag_) or (sentence[i].pos_ == \"VERB\" and \"<PS>\" in sentence[i].tag_) or (sentence[i].pos_ == \"VERB\" and \"<IMPF>\" in sentence[i].tag_):\n",
        "                return 1\n",
        "      return 0\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__pattern__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQynlG1sQCAf"
      },
      "source": [
        "# Lexicon features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joTrmOP3SfZE"
      },
      "outputs": [],
      "source": [
        "class LEX_Subjective(BaseEstimator):\n",
        "    def __init__(self, proportion):\n",
        "        self.name = \"PROP\" if proportion else \"NUM\" + \" SUBJECTIVE WORDS\"\n",
        "        self.file_name = liwc_path + \"LIWC_Portuguese_subjective.txt\"\n",
        "        self.lexicon = []\n",
        "        self.proportion = proportion\n",
        "\n",
        "    def __load_lexicon__(self):\n",
        "      with open(self.file_name, \"r\") as f:\n",
        "        self.lexicon = f.read().split('\\n')\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      count = 0\n",
        "      for term in sentence:\n",
        "        if term.text.lower() in self.lexicon:\n",
        "          count += 1\n",
        "        else:\n",
        "          for lexicon in self.lexicon:\n",
        "              if lexicon.endswith(\"*\") and term.text.lower().startswith(lexicon[:-1]):\n",
        "                 count += 1\n",
        "                 break\n",
        "      if self.proportion:\n",
        "          return count / len(sentence) if len(sentence) > 0 else 0\n",
        "      else:\n",
        "          return count\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      self.__load_lexicon__()\n",
        "      list_count = []\n",
        "      for doc in sentences:\n",
        "        list_count.append(self.__value__(doc))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV7FKA9o9p4a"
      },
      "outputs": [],
      "source": [
        "class LEX_Positive(BaseEstimator):\n",
        "    def __init__(self, proportion, categoria):\n",
        "        self.name = \"PROP\" if proportion else \"NUM\" + \" POSITIVE WORDS\"\n",
        "        self.categoria = categoria\n",
        "        self.lexicon = None\n",
        "        self.proportion = proportion\n",
        "\n",
        "    def __load_lexicon__(self):\n",
        "      self.file_name = lexico_path + f\"{self.categoria}.csv\"\n",
        "      self.lexicon = pd.read_csv(self.file_name)\n",
        "\n",
        "    def __value__(self, doc):\n",
        "      count = 0\n",
        "      for term in doc:\n",
        "        if self.lexicon[(self.lexicon['term'] == term.text) & (self.lexicon['class'] == 'positive')].any().any():\n",
        "          count += 1\n",
        "      if self.proportion:\n",
        "          return count / len(doc) if len(doc) > 0 else 0\n",
        "      else:\n",
        "          return count\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      self.__load_lexicon__()\n",
        "      list_count = []\n",
        "      for doc in sentences:\n",
        "        list_count.append(self.__value__(doc))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mErFR3JW9ujJ"
      },
      "outputs": [],
      "source": [
        "class LEX_Negative(BaseEstimator):\n",
        "    def __init__(self, proportion, categoria):\n",
        "        self.name = \"PROP\" if proportion else \"NUM\" + \"NEGATIVE WORDS\"\n",
        "        self.categoria = categoria\n",
        "        self.lexicon = None\n",
        "        self.proportion = proportion\n",
        "\n",
        "    def __load_lexicon__(self):\n",
        "      self.file_name = lexico_path + f\"{self.categoria}.csv\"\n",
        "      self.lexicon = pd.read_csv(self.file_name)\n",
        "\n",
        "    def __value__(self, doc):\n",
        "      count = 0\n",
        "      for term in doc:\n",
        "        if self.lexicon[(self.lexicon['term'] == term.text) & (self.lexicon['class'] == 'negative')].any().any():\n",
        "          count += 1\n",
        "      if self.proportion:\n",
        "          return count / len(doc) if len(doc) > 0 else 0\n",
        "      else:\n",
        "          return count\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        self.__load_lexicon__()\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__value__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1)  if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnsWlWe8JoAC"
      },
      "outputs": [],
      "source": [
        "class LEX_CountSentencesByPolarity(BaseEstimator):\n",
        "    def __init__(self, polarity, categoria):\n",
        "        self.name = f\"NUMBER OF + {polarity} + SENTENCES\"\n",
        "        self.categoria = categoria\n",
        "        self.polarity = polarity\n",
        "        self.nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        list_count = []\n",
        "        for document in documents:\n",
        "            count = 0\n",
        "            sentences = list(self.nlp(document).sents)\n",
        "            for sentence in sentences:\n",
        "              positive = LEX_Positive(proportion = False, categoria = self.categoria).transform([sentence])\n",
        "              negative = LEX_Negative(proportion = False, categoria = self.categoria).transform([sentence])\n",
        "              if self.polarity == 'positive':\n",
        "                if positive[0][0] > negative[0][0]:\n",
        "                  count += 1\n",
        "              if self.polarity == 'negative':\n",
        "                if negative[0][0] > positive[0][0]:\n",
        "                  count += 1\n",
        "            list_count.append(count)\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox_sOeaUCi2_"
      },
      "outputs": [],
      "source": [
        "class LEX_QuotationExclamation(BaseEstimator):\n",
        "    def __init__(self, proportion):\n",
        "        self.name = \"PROP\" if proportion else \"NUM\" + \"QUOTATION AND EXCLAMATION MARKS\"\n",
        "        self.proportion = proportion\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      if not sentence or sentence.text.isspace():\n",
        "        return 0\n",
        "      if self.proportion:\n",
        "        return len([term for term in sentence if term.text in [\"!\", \"?\"]])/len(sentence)\n",
        "      else:\n",
        "        return len([term for term in sentence if term.text in [\"!\", \"?\"]])\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      list_count = []\n",
        "      for doc in sentences:\n",
        "        list_count.append(self.__value__(doc))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttYnDqULqw57"
      },
      "source": [
        "# Concept-based Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFZBGOP1q6jO"
      },
      "outputs": [],
      "source": [
        "class CONC_Pleasantness(BaseEstimator):\n",
        "    def __init__(self, absolute, average):\n",
        "        self.name = \"AVERAGE\" if average else \"SUM\" + \" OF PLEASANTNESS SCORES\" + (\" (ABS)\" if absolute else \"\")\n",
        "        self.sn = SenticNet('pt')\n",
        "        self.absolute = absolute\n",
        "        self.average = average\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      total = 0\n",
        "      for word in sentence:\n",
        "        try:\n",
        "          if self.absolute:\n",
        "            score = abs(self.sn.sentics(word.text.lower())['pleasantness'])\n",
        "          else:\n",
        "            score = self.sn.sentics(word.text.lower())['pleasantness']\n",
        "          total += score\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if self.average:\n",
        "        return total/len(sentence)\n",
        "      else:\n",
        "        return total\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      list_count = []\n",
        "      for sentence in sentences:\n",
        "          list_count.append(self.__value__(sentence))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfhSviV064xL"
      },
      "outputs": [],
      "source": [
        "class CONC_Attention(BaseEstimator):\n",
        "    def __init__(self, absolute, average):\n",
        "        self.name = \"AVERAGE\" if average else \"SUM\" + \" OF ATTENTION SCORES\" + (\" (ABS)\" if absolute else \"\")\n",
        "        self.sn = SenticNet('pt')\n",
        "        self.absolute = absolute\n",
        "        self.average = average\n",
        "\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      total = 0\n",
        "      for word in sentence:\n",
        "        try:\n",
        "          if self.absolute:\n",
        "            score = abs(self.sn.sentics(word.text.lower())['attention'])\n",
        "          else:\n",
        "            score = self.sn.sentics(word.text.lower())['attention']\n",
        "          total += score\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if self.average:\n",
        "        return total/len(sentence)\n",
        "      else:\n",
        "        return total\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      list_count = []\n",
        "      for sentence in sentences:\n",
        "        list_count.append(self.__value__(sentence))\n",
        "\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mFFArtn7L9x"
      },
      "outputs": [],
      "source": [
        "class CONC_Sensitivity(BaseEstimator):\n",
        "    def __init__(self, absolute, average):\n",
        "        self.name = \"AVERAGE\" if average else \"SUM\" + \" OF SENSITIVITY SCORES\" + (\" (ABS)\" if absolute else \"\")\n",
        "        self.sn = SenticNet('pt')\n",
        "        self.absolute = absolute\n",
        "        self.average = average\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      total = 0\n",
        "      for word in sentence:\n",
        "        try:\n",
        "          if self.absolute:\n",
        "            score = abs(self.sn.sentics(word.text.lower())['sensitivity'])\n",
        "          else:\n",
        "            score = self.sn.sentics(word.text.lower())['sensitivity']\n",
        "          total += score\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if self.average:\n",
        "        return total/len(sentence)\n",
        "      else:\n",
        "        return total\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      list_count = []\n",
        "      for sentence in sentences:\n",
        "        list_count.append(self.__value__(sentence))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CU-19h3BvlX"
      },
      "outputs": [],
      "source": [
        "class CONC_Aptitude(BaseEstimator):\n",
        "    def __init__(self, absolute, average):\n",
        "        self.name = \"AVERAGE\" if average else \"SUM\" + \" OF APTITUDE SCORES\" + (\" (ABS)\" if absolute else \"\")\n",
        "        self.sn = SenticNet('pt')\n",
        "        self.absolute = absolute\n",
        "        self.average = average\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      total = 0\n",
        "      for word in sentence:\n",
        "        try:\n",
        "          if self.absolute:\n",
        "            score = abs(self.sn.sentics(word.text.lower())['aptitude'])\n",
        "          else:\n",
        "            score = self.sn.sentics(word.text.lower())['aptitude']\n",
        "          total += score\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if self.average:\n",
        "        return total/len(sentence)\n",
        "      else:\n",
        "        return total\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      list_count = []\n",
        "      for sentence in sentences:\n",
        "        list_count.append(self.__value__(sentence))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwkJEK23LmIJ"
      },
      "outputs": [],
      "source": [
        "class CONC_Polarity(BaseEstimator):\n",
        "    def __init__(self, absolute, average):\n",
        "        self.name = \"AVERAGE\" if average else \"SUM\" + \" OF POLARITY SCORES\" + (\" (ABS)\" if absolute else \"\")\n",
        "        self.sn = SenticNet('pt')\n",
        "        self.absolute = absolute\n",
        "        self.average= average\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "      total = 0\n",
        "      for word in sentence:\n",
        "        try:\n",
        "          if self.absolute:\n",
        "            score = abs(self.sn.polarity_value(word.text.lower()))\n",
        "          else:\n",
        "            score = self.sn.polarity_value(word.text.lower())\n",
        "          total += score\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if self.average:\n",
        "        return total/len(sentence)\n",
        "      else:\n",
        "        return total\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "      list_count = []\n",
        "      for sentence in sentences:\n",
        "        list_count.append(self.__value__(sentence))\n",
        "      return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3S_QC72xu_j"
      },
      "source": [
        "# Twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OH9QJR8l7jO"
      },
      "outputs": [],
      "source": [
        "class TWT_CountElongated(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"COUNT OF ELONGATED WORDS\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "          if not isinstance(doc, str):\n",
        "            doc = ' '.join([token.text for token in doc])\n",
        "          list_count.append(len(\n",
        "                re.findall(r\"([a-zA-z])\\1{2,}\", doc)\n",
        "            ))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LChoOqX7tsWx"
      },
      "outputs": [],
      "source": [
        "class TWT_CountExpressions(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"COUNT EXPRESSIONS (GÍRIAS)\"\n",
        "        self.file_name = girias_dir\n",
        "        self.lexicon = []\n",
        "        self.nlp = SpacyPreprocessor().nlp\n",
        "        self.matcher = PhraseMatcher(self.nlp.vocab)\n",
        "\n",
        "    def __load_lexicon__(self):\n",
        "        with open(self.file_name, \"r\") as f:\n",
        "            self.lexicon = f.read().split('\\n')\n",
        "            self.lexicon = [term.lower() for term in self.lexicon]\n",
        "            patterns = self.nlp.pipe(self.lexicon)\n",
        "            self.matcher.add(\"Girias\", patterns)\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "        doc = self.nlp(sentence)\n",
        "        return len(self.matcher(doc))\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        self.__load_lexicon__()\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__value__(doc.text.lower()))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toLmAvUkujeR"
      },
      "outputs": [],
      "source": [
        "class TWT_Negation(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"PRESENCE OF NEGATION WORDS\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            presence = 0\n",
        "            for token in doc:\n",
        "                if token.text in ['não', 'nao', 'nem', 'nunca', 'nn', 'n', 'ñ']:\n",
        "                    presence = 1\n",
        "                    break\n",
        "            list_count.append(presence)\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y19DyEBcmCbG"
      },
      "outputs": [],
      "source": [
        "class TWT_EmojiPolarityScore(BaseEstimator):\n",
        "    def __init__(self, scoring_type='polarity'):\n",
        "        self.scoring_type = scoring_type\n",
        "        self.name = \"SUM OF \" + scoring_type.upper() + \" EMOJIS\"\n",
        "        self.file_name = emoji_dir\n",
        "        self.emoji_data = None\n",
        "\n",
        "    def __get_emojis__(self, sentence):\n",
        "        pass\n",
        "\n",
        "    def __load_lexicon__(self):\n",
        "        with open(self.file_name, \"r\") as f:\n",
        "            self.emoji_data = pd.read_csv(self.file_name, index_col='char').to_dict('index')\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "        score = 0\n",
        "        for term in sentence:\n",
        "            if term.text in self.emoji_data:\n",
        "                score += self.emoji_data[term.text][self.scoring_type]\n",
        "        return score\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        self.__load_lexicon__()\n",
        "        list_count = []\n",
        "        for sentence in sentences:\n",
        "            list_count.append(self.__value__(sentence))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "One8416ymjt5"
      },
      "outputs": [],
      "source": [
        "class TWT_EmoticonPolarityScore(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"SUM OF EMOTICON SCORE\"\n",
        "        self.file_name = emoticon_dir\n",
        "        self.emoticon_data = None\n",
        "\n",
        "    def __get_emojis__(self, sentence):\n",
        "        pass\n",
        "\n",
        "    def __load_lexicon__(self):\n",
        "        with open(self.file_name, \"r\") as f:\n",
        "            self.emoticon_data = pd.read_csv(self.file_name, sep='\\t', index_col='emoticon').to_dict('index')\n",
        "\n",
        "    def __value__(self, sentence):\n",
        "        score = 0\n",
        "        for term in sentence:\n",
        "            if term.text in self.emoticon_data:\n",
        "                score += self.emoticon_data[term.text]['sentiment']\n",
        "        return score\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        self.__load_lexicon__()\n",
        "        list_count = []\n",
        "        for sentence in sentences:\n",
        "            list_count.append(self.__value__(sentence))\n",
        "        return np.array(list_count).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQJ1B_dKwlai"
      },
      "source": [
        "# Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI4eT_mLwT_F"
      },
      "outputs": [],
      "source": [
        "class SBJ_CountNE(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"COUNT OF NAMED ENTITIES\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(len(doc.ents))\n",
        "        return np.array(list_count).reshape(-1,1) if list_count else np.zeros((len(sentences), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D1kGggKpVvc",
        "outputId": "66e5f4b3-d2c6-4474-bc5e-d1ffbd3d260e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass SBJ_FutureTense(BaseEstimator):\\n    def __init__(self):\\n        self.name = \"PRESENCE OF FUTURE TENSE VERBS IN THE SENTENCE\"\\n\\n    def fit(self, X=None, y=None):\\n        return self\\n\\n    def __is_future(self, tag):\\n        return \\'Tense=Fut\\' in tag.split(\\'|\\')\\n\\n    def transform(self, sentences):\\n        list_count = []\\n        for doc in sentences:\\n            presence = 0\\n            for token in doc:\\n                if (token.pos_ == \\'VERB\\' or token.pos_ == \\'AUX\\') and self.__is_future(token.tag_):\\n                    presence = 1\\n                    break\\n            list_count.append(presence)\\n        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))\\n'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# feature retirada\n",
        "'''\n",
        "class SBJ_FutureTense(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"PRESENCE OF FUTURE TENSE VERBS IN THE SENTENCE\"\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def __is_future(self, tag):\n",
        "        return 'Tense=Fut' in tag.split('|')\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            presence = 0\n",
        "            for token in doc:\n",
        "                if (token.pos_ == 'VERB' or token.pos_ == 'AUX') and self.__is_future(token.tag_):\n",
        "                    presence = 1\n",
        "                    break\n",
        "            list_count.append(presence)\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znPzlk6cyxWc"
      },
      "outputs": [],
      "source": [
        "class SBJ_CountCorrectWords(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.name = \"COUNT CORRECT WORDS\"\n",
        "        self.hunspell = hunspell.HunSpell(dic_path, aff_path)\n",
        "\n",
        "    def __value__(self, doc):\n",
        "        count_correct = 0\n",
        "        for token in doc:\n",
        "            if self.hunspell.spell(token.text) and token.text.isalpha(): \n",
        "                count_correct += 1\n",
        "        return count_correct\n",
        "\n",
        "    def fit(self, x=None, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, sentences):\n",
        "        list_count = []\n",
        "        for doc in sentences:\n",
        "            list_count.append(self.__value__(doc))\n",
        "        return np.array(list_count).reshape(-1, 1) if list_count else np.zeros((len(sentences), 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNx8lsWWFv7j"
      },
      "source": [
        "# Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STjIeFDbMoEn"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(steps=[\n",
        "      ('features', FeatureUnion(\n",
        "          transformer_list=[\n",
        "              ('qtCharacters', CountCharacters()),\n",
        "              ('qtSentences', CountSentences()),\n",
        "              ('qtWords', CountWords()),\n",
        "              ('qtCapitalizedWords',CountWordsWithUpperLetter()),\n",
        "              ('qtCapitalizedChars',CountUpperLetters()),\n",
        "              ('propCapitalizedWords',ProportionCapitalizedWords()),\n",
        "              ('propCapitalizedChars',ProportionCapitalizedChars()),\n",
        "              ('tree', Pipeline([\n",
        "                ('spacy', SpacyPreprocessor()),\n",
        "                ('tree_features', FeatureUnion(\n",
        "                    transformer_list=[\n",
        "                      ('POS_adjectives', POS_CountAdj()),\n",
        "                      ('POS_adp', POS_CountAdp()),\n",
        "                      ('POS_adv', POS_CountAdv()),\n",
        "                      ('POS_aux', POS_CountAux()),\n",
        "                      ('POS_cconj', POS_CountCconj()),\n",
        "                      ('POS_det', POS_CountDet()),\n",
        "                      ('POS_intj', POS_CountIntj()),\n",
        "                      ('POS_noun', POS_CountNoun()),\n",
        "                      ('POS_num', POS_CountNum()),\n",
        "                      ('POS_part', POS_CountPart()),\n",
        "                      ('POS_pron', POS_CountPron()),\n",
        "                      ('POS_propn', POS_CountPropn()),\n",
        "                      ('POS_punct', POS_CountPunct()),\n",
        "                      ('POS_sconj', POS_CountSconj()),\n",
        "                      ('POS_sym', POS_CountSym()),\n",
        "                      ('POS_verb', POS_CountVerb()),\n",
        "                      ('POS_comp',POS_CountComparatives()),\n",
        "                      ('POS_sup',POS_CountSuperlatives()),\n",
        "                      ('POS_x', POS_CountX()),\n",
        "                      ('SYNT_1', SYNT_Rule1()),\n",
        "                      ('SYNT_2', SYNT_Rule2()),\n",
        "                      ('SYNT_3', SYNT_Rule3()),\n",
        "                      ('SYNT_4', SYNT_Rule4()),\n",
        "                      ('SYNT_5', SYNT_Rule5()),\n",
        "                      ('LEX_Subjective', LEX_Subjective(proportion = False)),\n",
        "                      ('LEX_PropSubjective', LEX_Subjective(proportion = True)),\n",
        "                      ('LEX_Positive', LEX_Positive(proportion = False, categoria = None)),\n",
        "                      ('LEX_PropPositive', LEX_Positive(proportion = True, categoria = None)),\n",
        "                      ('LEX_Negative', LEX_Negative(proportion = False, categoria = None)),\n",
        "                      ('LEX_PropNegative', LEX_Negative(proportion = True, categoria = None)),\n",
        "                      ('LEX_CountNegativeSentences',LEX_CountSentencesByPolarity(polarity = 'negative', categoria = None)),\n",
        "                      ('LEX_CountPositiveSentences',LEX_CountSentencesByPolarity(polarity = 'positive', categoria = None)),\n",
        "                      ('LEX_QuotationExclamation', LEX_QuotationExclamation(proportion = False)),\n",
        "                      ('LEX_PropQuotationExclamation', LEX_QuotationExclamation(proportion = True)),\n",
        "                      ('CONC_Pleasantness', CONC_Pleasantness(absolute=True, average = False)),\n",
        "                      ('CONC_AvgPleasantness', CONC_Pleasantness(absolute=True, average = True)),\n",
        "                      ('CONC_Attention', CONC_Attention(absolute=True, average = False)),\n",
        "                      ('CONC_AvgAttention', CONC_Attention(absolute=True, average = True)),\n",
        "                      ('CONC_Sensitivity', CONC_Sensitivity(absolute=True, average = False)),\n",
        "                      ('CONC_AvgSensitivity', CONC_Sensitivity(absolute=True, average = True)),\n",
        "                      ('CONC_Aptitude', CONC_Aptitude(absolute=True, average = True)),\n",
        "                      ('CONC_AvgAptitude', CONC_Aptitude(absolute=True, average = False)),\n",
        "                      ('CONC_Polarity', CONC_Polarity(absolute=True, average = False)),\n",
        "                      ('CONC_AvgPolarity', CONC_Polarity(absolute=True, average = True)),\n",
        "                      ('TWT_CountElongated', TWT_CountElongated()),\n",
        "                      ('TWT_CountExpressions',TWT_CountExpressions()),\n",
        "                      ('TWT_Negation',TWT_Negation()),\n",
        "                      ('TWT_EmojiPolarityScore', TWT_EmojiPolarityScore()),\n",
        "                      ('TWT_EmoticonPolarityScore', TWT_EmoticonPolarityScore()),\n",
        "                      ('SBJ_CountNE', SBJ_CountNE()),\n",
        "                     # ('SBJ_FutureTense', SBJ_FutureTense()), feature retirada\n",
        "                      ('SBJ_CountCorrectWords',SBJ_CountCorrectWords()),\n",
        "                ]))\n",
        "            ]))\n",
        "          ],\n",
        "      ))\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGjsuiTn7fxb"
      },
      "outputs": [],
      "source": [
        "def dataf(categoria):\n",
        "  dataframes = []\n",
        "  dir = f'{extract_path}/amazon_original_dataset/{categoria}/'\n",
        "  for arquivo in os.listdir(dir):\n",
        "    arquivo_path = dir+arquivo\n",
        "    if arquivo_path.endswith('.json'):\n",
        "        dataframe = pd.read_json(arquivo_path)\n",
        "        dataframes.append(dataframe)\n",
        "  data = pd.concat(dataframes, ignore_index=True)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6nw8HjhdAb",
        "outputId": "f4b1bba7-236d-49e4-856d-fffebbeceb1e"
      },
      "outputs": [],
      "source": [
        "categories = ['auto', 'baby','celular','food','games','laptops','livros','moda','pets','toys']\n",
        "\n",
        "for categoria in categories:\n",
        "  data = dataf(categoria)\n",
        "  data['categoria'] = categoria\n",
        "  print(f\"categoria: {categoria} - {data.shape}\")\n",
        "  pipeline.set_params(\n",
        "        features__tree__tree_features__LEX_Positive__categoria=categoria,\n",
        "        features__tree__tree_features__LEX_PropPositive__categoria=categoria,\n",
        "        features__tree__tree_features__LEX_Negative__categoria=categoria,\n",
        "        features__tree__tree_features__LEX_PropNegative__categoria=categoria,\n",
        "        features__tree__tree_features__LEX_CountNegativeSentences__categoria=categoria,\n",
        "        features__tree__tree_features__LEX_CountPositiveSentences__categoria=categoria,\n",
        "    )\n",
        "  features = pipeline.transform(data['text']).tolist()\n",
        "  num_samples = len(features)\n",
        "  num_features = len(features[0])\n",
        "  print(f\"Dimensões das features: ({num_samples}, {num_features})\")\n",
        "  os.makedirs(f\"{extract_path}/features_extracted/{categoria}\", exist_ok=True)\n",
        "  with open(f\"{extract_path}/features_extracted/{categoria}/{categoria}.csv\", 'w') as arquivo:\n",
        "    columns = ['text', 'rating'] + feature_names\n",
        "    write = csv.DictWriter(arquivo, fieldnames=columns, delimiter=',', lineterminator='\\n')\n",
        "    write.writeheader()\n",
        "    for i, sentence in enumerate(data['text']):\n",
        "      row = {\n",
        "          'text': sentence,\n",
        "          'rating': data['rating'].iloc[i]\n",
        "      }\n",
        "      for j, feature_name in enumerate(feature_names):\n",
        "          row[feature_name] = features[i][j]\n",
        "      write.writerow(row)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sYtu9iBHC-sL",
        "n_Xe5dM339G-",
        "jLMXUC_V4Jvx",
        "mQynlG1sQCAf",
        "ttYnDqULqw57",
        "q3S_QC72xu_j",
        "DQJ1B_dKwlai"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
